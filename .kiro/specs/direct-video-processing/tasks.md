# Direct Video Processing Implementation Tasks (Option 3)

## Overview

This document breaks down the implementation into concrete, testable tasks. Each task is self-contained and can be verified independently. All work happens in Node.js backend - no Flask wrapper needed.

## Phase 1: Video Processing Service

### Task 1.1: Create Video Processing Service

**Objective:** Create service to spawn `track.py` subprocess and capture output

**Acceptance Criteria:**
- [ ] Create file: `backend/src/services/videoProcessingService.ts`
- [ ] Export function: `processVideoWithTrackPy(videoPath: string, timeout?: number)`
- [ ] Spawn subprocess with command: `python track.py video.source={videoPath}`
- [ ] Set working directory to `/home/ben/pose-service/4D-Humans`
- [ ] Capture stdout and stderr
- [ ] Set timeout to 180 seconds (configurable)
- [ ] Return object with: `{ success, exitCode, stdout, stderr, pklPath, error }`
- [ ] Handle TimeoutError and return error response
- [ ] Handle subprocess crash and return error response
- [ ] Log subprocess output for debugging

**Implementation Notes:**
- Use Node.js `child_process.spawn()` or `execFile()`
- Wrap in Promise for async/await
- Detect .pkl file location from stdout or search filesystem
- Log timing for performance analysis

**Testing:**
```bash
# Test with valid video
const result = await processVideoWithTrackPy('/tmp/videos/test.mp4');
console.log(result.success);  // true
console.log(result.pklPath);  // /tmp/phalp_output/results.pkl
```

---

### Task 1.2: Implement Pickle File Detection

**Objective:** Locate the `.pkl` output file generated by track.py

**Acceptance Criteria:**
- [ ] After subprocess completes, search for `.pkl` file
- [ ] Search in `/tmp/phalp_output` directory (or track.py's default output)
- [ ] Use glob pattern to find `.pkl` files
- [ ] Return error if no `.pkl` file found
- [ ] Return error if multiple `.pkl` files found (ambiguous)
- [ ] Successfully identify single `.pkl` file
- [ ] Return full path to `.pkl` file
- [ ] Log file path for debugging

**Implementation Notes:**
- Use Node.js `glob` package or `fs.readdirSync()`
- Check track.py documentation for output location
- May need to configure track.py output path via environment variable
- Handle race conditions (file still being written)

**Testing:**
```bash
# After subprocess completes, verify .pkl exists
ls -la /tmp/phalp_output/
```

---

## Phase 2: Pickle Parsing Service

### Task 2.1: Create Pickle Parser Service

**Objective:** Parse PHALP's `.pkl` output into JSON format

**Acceptance Criteria:**
- [ ] Create file: `backend/src/services/pickleParserService.ts`
- [ ] Export function: `parsePickleToFrames(pklPath: string)`
- [ ] Load `.pkl` file using Python subprocess (pickle module)
- [ ] Extract frame count from pickle data
- [ ] For each frame, extract:
  - [ ] Frame number and timestamp
  - [ ] Person detections (track_id, confidence)
  - [ ] SMPL parameters (betas, body_pose, global_orient)
  - [ ] 3D keypoints (45 joints)
  - [ ] 2D keypoints (45 joints)
  - [ ] Camera parameters (tx, ty, tz, focal_length)
  - [ ] Bounding box (x0, y0, w, h)
- [ ] Compute mesh vertices from SMPL parameters (6890 vertices)
- [ ] Compute mesh faces (13776 faces)
- [ ] Convert all numpy arrays to JavaScript arrays (JSON serializable)
- [ ] Return array of FrameData objects
- [ ] Handle parsing errors gracefully

**Implementation Notes:**
- Use Python subprocess to load pickle (Node.js has no native pickle support)
- Create helper Python script: `backend/src/services/pickle_parser.py`
- SMPL model computation can use existing Python libraries
- Ensure no numpy types in output (convert to plain JS arrays)
- Handle ghost detections (tracking_confidence < 1.0)

**Testing:**
```typescript
// Test JSON serialization
const frames = await parsePickleToFrames(pklPath);
console.log(frames.length);  // Should be > 0
console.log(frames[0].frameNumber);  // 0
console.log(frames[0].persons[0].meshVertices.length);  // 6890
```

---

### Task 2.2: Create Python Pickle Parser Helper

**Objective:** Create Python script to parse pickle and output JSON

**Acceptance Criteria:**
- [ ] Create file: `backend/src/services/pickle_parser.py`
- [ ] Accept pickle file path as command-line argument
- [ ] Load pickle file using `pickle.load()`
- [ ] Extract frame data structure
- [ ] For each frame:
  - [ ] Extract frame number, timestamp
  - [ ] Extract person data (track_id, confidence, etc.)
  - [ ] Extract SMPL parameters
  - [ ] Extract keypoints (3D and 2D)
  - [ ] Extract camera parameters
  - [ ] Extract bounding box
  - [ ] Compute mesh vertices from SMPL params
  - [ ] Compute mesh faces
- [ ] Convert all numpy arrays to Python lists
- [ ] Output JSON to stdout
- [ ] Handle errors and output to stderr
- [ ] Exit with code 0 on success, 1 on error

**Implementation Notes:**
- Use `json.dumps()` for output
- Use SMPL model to compute vertices (from 4D-Humans codebase)
- Handle both dict and list pickle structures
- Ensure all output is JSON-serializable (no numpy types)

**Testing:**
```bash
python backend/src/services/pickle_parser.py /tmp/phalp_output/results.pkl | jq '.frames | length'
```

---

## Phase 3: MongoDB Frame Storage

### Task 3.1: Create Frame Storage Service

**Objective:** Store frames in MongoDB with proper schema and indexes

**Acceptance Criteria:**
- [ ] Create file: `backend/src/services/frameStorageService.ts`
- [ ] Export function: `storeFrames(videoId: string, frames: FrameData[])`
- [ ] Create MongoDB collection: `frames`
- [ ] For each frame, create document with:
  - [ ] videoId, frameNumber, timestamp
  - [ ] persons array (with all frame data)
  - [ ] createdAt, updatedAt timestamps
- [ ] Use batch insert for performance (`insertMany()`)
- [ ] Create indexes:
  - [ ] `{videoId: 1, frameNumber: 1}` (primary query)
  - [ ] `{videoId: 1}` (list all frames)
  - [ ] `{createdAt: 1}` with TTL 30 days
- [ ] Handle duplicate key errors gracefully
- [ ] Return success/error response

**Implementation Notes:**
- Use MongoDB Node.js driver
- Batch insert all frames at once (not one-by-one)
- Create indexes on first run (idempotent)
- Add error handling for connection failures

**Testing:**
```javascript
// Query single frame
db.frames.findOne({videoId: "v_123", frameNumber: 0})

// Query all frames for video
db.frames.find({videoId: "v_123"}).sort({frameNumber: 1})

// Verify document size < 16 MB
```

---

### Task 3.2: Create Frame Query Service

**Objective:** Query frames from MongoDB for frontend

**Acceptance Criteria:**
- [ ] Create file: `backend/src/services/frameQueryService.ts`
- [ ] Export function: `getFrame(videoId: string, frameNumber: number)`
- [ ] Export function: `getAllFrames(videoId: string)`
- [ ] Export function: `getFrameRange(videoId: string, start: number, end: number)`
- [ ] Return frame data with all mesh information
- [ ] Handle not found gracefully (return null)
- [ ] Handle database errors gracefully

**Implementation Notes:**
- Use MongoDB queries with proper indexes
- Return only necessary fields (optimize for frontend)
- Consider pagination for large videos

**Testing:**
```typescript
const frame = await getFrame('v_123', 0);
console.log(frame.persons[0].meshVertices.length);  // 6890

const allFrames = await getAllFrames('v_123');
console.log(allFrames.length);  // Total frame count
```

---

## Phase 4: Backend Integration

### Task 4.1: Modify finalize-upload Endpoint

**Objective:** Integrate video processing into upload flow

**Acceptance Criteria:**
- [ ] Modify file: `backend/src/server.ts` - `/api/finalize-upload` endpoint
- [ ] After assembling chunks into video file:
  - [ ] Generate videoId
  - [ ] Save video to disk: `/tmp/videos/{videoId}.mp4`
  - [ ] Spawn track.py subprocess via `processVideoWithTrackPy()`
  - [ ] If subprocess fails, return error response
  - [ ] Parse .pkl output via `parsePickleToFrames()`
  - [ ] If parsing fails, return error response
  - [ ] Store frames in MongoDB via `storeFrames()`
  - [ ] If storage fails, return error response
  - [ ] Return success response with videoId and frameCount
- [ ] Add error handling for each step
- [ ] Log timing for performance analysis
- [ ] Log all errors for debugging

**Implementation Notes:**
- Use async/await for clean flow
- Add try/catch blocks for error handling
- Log detailed error messages
- Return appropriate HTTP status codes

**Testing:**
```bash
# Upload video via API
curl -X POST http://localhost:3001/api/finalize-upload \
  -H "Content-Type: application/json" \
  -d '{"role": "user", "sessionId": "123", "filename": "test.mp4", "filesize": 1000000}'

# Should return: { success: true, videoId: "v_...", frameCount: 140 }
```

---

### Task 4.2: Create Frame Retrieval Endpoints

**Objective:** Create API endpoints for frontend to retrieve frame data

**Acceptance Criteria:**
- [ ] Create endpoint: `GET /api/frames/:videoId`
  - [ ] Returns all frames for video
  - [ ] Returns array of frame objects
  - [ ] Includes mesh data (vertices, faces, camera)
- [ ] Create endpoint: `GET /api/frames/:videoId/:frameNumber`
  - [ ] Returns single frame
  - [ ] Includes all mesh data
  - [ ] Returns 404 if not found
- [ ] Create endpoint: `GET /api/frames/:videoId/range?start=0&end=10`
  - [ ] Returns frame range
  - [ ] Useful for pagination
- [ ] Add error handling for all endpoints
- [ ] Add logging for debugging

**Implementation Notes:**
- Use frameQueryService for database access
- Return JSON responses
- Add proper HTTP status codes

**Testing:**
```bash
# Get all frames for video
curl http://localhost:3001/api/frames/v_123

# Get single frame
curl http://localhost:3001/api/frames/v_123/0

# Get frame range
curl http://localhost:3001/api/frames/v_123/range?start=0&end=10
```

---

## Phase 5: Error Handling & Fallback

### Task 5.1: Implement Error Handling

**Objective:** Handle all error cases gracefully

**Acceptance Criteria:**
- [ ] If video file is invalid, return HTTP 400 with error details
- [ ] If subprocess times out, return HTTP 500 with "Processing timeout"
- [ ] If subprocess crashes, return HTTP 500 with stderr output
- [ ] If .pkl file not found, return HTTP 500 with error
- [ ] If .pkl parsing fails, return HTTP 500 with error
- [ ] If MongoDB storage fails, return HTTP 500 with error
- [ ] All errors logged with full details
- [ ] Backend does NOT crash on any error
- [ ] User receives meaningful error messages

**Implementation Notes:**
- Use try/catch blocks
- Log full error stack traces
- Return appropriate HTTP status codes
- Include error details in response (for debugging)

**Testing:**
```bash
# Test with invalid video path
curl -X POST http://localhost:3001/api/finalize-upload \
  -d '{"videoPath": "/nonexistent/video.mp4"}'
# Should return 400 error

# Test with timeout (simulate slow GPU)
# Should return 500 with "Processing timeout"
```

---

### Task 5.2: Implement Fallback to Frame-by-Frame (Optional)

**Objective:** Fall back to frame-by-frame processing if direct processing fails

**Acceptance Criteria:**
- [ ] If direct processing fails, catch error
- [ ] Log warning message
- [ ] Fall back to frame-by-frame processing
- [ ] Extract frames from video
- [ ] Call `/api/pose/hybrid` for each frame
- [ ] Store results in MongoDB
- [ ] Return success response

**Implementation Notes:**
- Wrap direct processing in try/catch
- Reuse existing frame extraction code
- Log fallback reason for debugging
- This is optional for MVP (can skip if direct processing works)

**Testing:**
```bash
# Simulate direct processing failure
# Verify fallback to frame-by-frame
# Verify results are stored in MongoDB
```

---

## Phase 6: Testing & Validation

### Task 6.1: Write Unit Tests

**Objective:** Test individual components

**Acceptance Criteria:**
- [ ] Test videoProcessingService:
  - [ ] Subprocess spawning
  - [ ] Timeout handling
  - [ ] Error handling
  - [ ] Exit code checking
- [ ] Test pickleParserService:
  - [ ] Pickle loading
  - [ ] Frame extraction
  - [ ] SMPL parameter parsing
  - [ ] Numpy array conversion
- [ ] Test frameStorageService:
  - [ ] Frame document creation
  - [ ] Index creation
  - [ ] Query by videoId + frameNumber
  - [ ] Query all frames
- [ ] Test error handling:
  - [ ] Invalid video path
  - [ ] Missing .pkl file
  - [ ] Corrupted pickle
  - [ ] Subprocess timeout
  - [ ] MongoDB connection failure

**Implementation Notes:**
- Use Jest or Mocha for testing
- Mock subprocess calls
- Mock MongoDB calls
- Test both success and error cases

**Testing:**
```bash
npm test -- backend/src/services/videoProcessingService.test.ts
npm test -- backend/src/services/pickleParserService.test.ts
npm test -- backend/src/services/frameStorageService.test.ts
```

---

### Task 6.2: Write Property-Based Tests

**Objective:** Test universal properties across all inputs

**Acceptance Criteria:**
- [ ] Property 1: Frame coverage
  - For any video with N frames, output SHALL contain exactly N frames
  - **Validates: Requirement 3.1**
- [ ] Property 2: Track ID consistency
  - For any person track, track_id SHALL be consistent across all frames
  - **Validates: Requirement 6.3**
- [ ] Property 3: Mesh vertex count
  - For any frame with persons, each person SHALL have exactly 6890 vertices
  - **Validates: Requirement 3.3**
- [ ] Property 4: Mesh face count
  - For any frame with persons, each person SHALL have exactly 13776 faces
  - **Validates: Requirement 3.3**
- [ ] Property 5: Keypoint count
  - For any frame with persons, each person SHALL have exactly 45 3D and 45 2D keypoints
  - **Validates: Requirement 3.3**
- [ ] Property 6: Temporal smoothness
  - For any two consecutive frames with same person, pose change SHALL be smooth
  - **Validates: Requirement 6.1**

**Implementation Notes:**
- Use fast-check or similar PBT library
- Generate random video data
- Test properties across many inputs
- Run minimum 100 iterations per property

**Testing:**
```bash
npm test -- backend/src/services/videoProcessingService.pbt.ts
```

---

### Task 6.3: End-to-End Testing

**Objective:** Test complete flow from upload to rendering

**Acceptance Criteria:**
- [ ] Upload test video via `/api/finalize-upload`
- [ ] Verify video is saved to disk
- [ ] Verify track.py subprocess runs
- [ ] Verify .pkl file is created
- [ ] Verify frames are extracted
- [ ] Verify frames are stored in MongoDB
- [ ] Query frames via `/api/frames/:videoId`
- [ ] Verify frame data is complete (vertices, faces, camera)
- [ ] Verify frame count matches video
- [ ] Verify temporal coherence (smooth motion)

**Implementation Notes:**
- Use test video (short, ~30 seconds)
- Monitor logs for debugging
- Check MongoDB for stored frames
- Verify Three.js can render mesh

**Testing:**
```bash
# Upload test video
curl -X POST http://localhost:3001/api/finalize-upload \
  -F "video=@test_video.mp4"

# Query frames
curl http://localhost:3001/api/frames/v_123

# Verify in MongoDB
db.frames.find({videoId: "v_123"}).count()
```

---

## Phase 7: Performance & Optimization

### Task 7.1: Performance Benchmarking

**Objective:** Measure and optimize performance

**Acceptance Criteria:**
- [ ] Measure subprocess spawn time (target: 2-3s)
- [ ] Measure track.py execution time (target: 60-120s for 140 frames)
- [ ] Measure pickle parsing time (target: 1-2s)
- [ ] Measure MongoDB storage time (target: 1-2s)
- [ ] Measure total end-to-end time (target: <2 minutes)
- [ ] Log timing for each phase
- [ ] Identify bottlenecks
- [ ] Document results

**Implementation Notes:**
- Use `console.time()` and `console.timeEnd()`
- Log timing at each phase
- Test with 140-frame video
- Test with GPU enabled

**Testing:**
```bash
# Run with timing logs
npm run dev

# Upload test video and monitor logs
# Should see timing for each phase
```

---

### Task 7.2: Optimize Pickle Parsing

**Objective:** Optimize pickle parsing performance

**Acceptance Criteria:**
- [ ] Profile pickle parsing
- [ ] Identify slow operations
- [ ] Optimize frame extraction
- [ ] Optimize numpy array conversion
- [ ] Optimize mesh vertex computation
- [ ] Measure improvement
- [ ] Document optimizations

**Implementation Notes:**
- Use Python profiler
- Consider streaming parsing (not all at once)
- Consider caching SMPL model
- Consider batch operations

**Testing:**
```bash
# Profile pickle parsing
python -m cProfile backend/src/services/pickle_parser.py /tmp/phalp_output/results.pkl
```

---

## Phase 8: Configuration & Deployment

### Task 8.1: Add Configuration

**Objective:** Make backend configurable for deployment

**Acceptance Criteria:**
- [ ] Add environment variables:
  - [ ] `TRACK_PY_PATH` - Path to track.py
  - [ ] `TRACK_PY_WORKING_DIR` - Working directory
  - [ ] `TRACK_PY_TIMEOUT` - Timeout in milliseconds
  - [ ] `VIDEO_STORAGE_PATH` - Video storage path
  - [ ] `MONGODB_URI` - MongoDB connection string
- [ ] Add defaults for development
- [ ] Load from `.env.local` file
- [ ] Validate configuration on startup
- [ ] Log configuration on startup

**Implementation Notes:**
- Use dotenv package
- Add to `.env.local` (not committed)
- Validate paths exist
- Validate timeout is reasonable

**Testing:**
```bash
# Check configuration
echo $TRACK_PY_PATH
echo $TRACK_PY_WORKING_DIR
echo $TRACK_PY_TIMEOUT
```

---

### Task 8.2: Add Monitoring & Logging

**Objective:** Monitor video processing for production

**Acceptance Criteria:**
- [ ] Log subprocess spawn success/failure
- [ ] Log processing time per video
- [ ] Log error rates
- [ ] Log MongoDB storage success/failure
- [ ] Log frame count per video
- [ ] Log all errors with full details
- [ ] Add metrics for monitoring

**Implementation Notes:**
- Use existing logger service
- Log at appropriate levels (info, warn, error)
- Include context (videoId, frameCount, timing)
- Consider adding metrics (Prometheus, etc.)

**Testing:**
```bash
# Monitor logs during video processing
tail -f backend/logs/app.log
```

---

## Task Dependencies

```
Phase 1: Video Processing Service
├─ Task 1.1: Create video processing service
└─ Task 1.2: Implement pickle file detection

Phase 2: Pickle Parsing Service
├─ Task 2.1: Create pickle parser service
└─ Task 2.2: Create Python pickle parser helper

Phase 3: MongoDB Frame Storage
├─ Task 3.1: Create frame storage service
└─ Task 3.2: Create frame query service

Phase 4: Backend Integration
├─ Task 4.1: Modify finalize-upload endpoint
└─ Task 4.2: Create frame retrieval endpoints

Phase 5: Error Handling & Fallback
├─ Task 5.1: Implement error handling
└─ Task 5.2: Implement fallback (optional)

Phase 6: Testing & Validation
├─ Task 6.1: Write unit tests
├─ Task 6.2: Write property-based tests
└─ Task 6.3: End-to-end testing

Phase 7: Performance & Optimization
├─ Task 7.1: Performance benchmarking
└─ Task 7.2: Optimize pickle parsing

Phase 8: Configuration & Deployment
├─ Task 8.1: Add configuration
└─ Task 8.2: Add monitoring & logging
```

## Success Criteria Summary

- [ ] Subprocess spawning implemented and functional
- [ ] `.pkl` output parsed to JSON format
- [ ] 100% frame coverage achieved (0 frames lost)
- [ ] Temporal coherence verified (smooth motion in rendered mesh)
- [ ] Output format compatible with MongoDB schema
- [ ] Backend integration complete (finalize-upload uses direct processing)
- [ ] Error handling robust (no backend crashes)
- [ ] Multi-person tracking produces consistent track IDs
- [ ] Processing time under 2 minutes for 140-frame video with GPU
- [ ] All frames stored in MongoDB and queryable by frontend
- [ ] Frame retrieval endpoints working
- [ ] Unit tests passing
- [ ] Property-based tests passing
- [ ] End-to-end testing successful
- [ ] Performance benchmarks documented
- [ ] Configuration complete and documented
- [ ] Monitoring and logging in place

