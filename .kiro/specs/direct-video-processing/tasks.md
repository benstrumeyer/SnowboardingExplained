# Direct Video Processing Implementation Tasks (Option 3)

## Overview

This document breaks down the implementation into concrete, testable tasks. Each task is self-contained and can be verified independently. All work happens in Node.js backend - no Flask wrapper needed.

**Frontend Integration:** Frontend polls database for all processed meshes and displays them in a side-by-side layout with original video, overlay video, and Three.js 3D mesh visualization.

**Logging:** Aggressive console logging at each step for debugging and progress tracking.

## Phase 1: Video Processing Service

### Task 1.1: Create Video Processing Service

**Objective:** Create service to spawn `track.py` subprocess and capture output

**Acceptance Criteria:**
- [ ] Create file: `backend/src/services/videoProcessingService.ts`
- [ ] Export function: `processVideoWithTrackPy(videoPath: string, timeout?: number)`
- [ ] Spawn subprocess with command: `python track.py video.source={videoPath}`
- [ ] Set working directory to `/home/ben/pose-service/4D-Humans`
- [ ] Capture stdout and stderr
- [ ] Set timeout to 180 seconds (configurable)
- [ ] Return object with: `{ success, exitCode, stdout, stderr, pklPath, error }`
- [ ] Handle TimeoutError and return error response
- [ ] Handle subprocess crash and return error response
- [ ] Log subprocess output for debugging

**Implementation Notes:**
- Use Node.js `child_process.spawn()` or `execFile()`
- Wrap in Promise for async/await
- Detect .pkl file location from stdout or search filesystem
- Log timing for performance analysis

**Testing:**
```bash
# Test with valid video
const result = await processVideoWithTrackPy('/tmp/videos/test.mp4');
console.log(result.success);  // true
console.log(result.pklPath);  // /tmp/phalp_output/results.pkl
```

---

### Task 1.3: Save track.py Output Video

**Objective:** Save the mesh overlay video generated by track.py

**Acceptance Criteria:**
- [ ] After track.py completes, locate the output video file
- [ ] track.py outputs video with mesh overlay (usually `output.mp4` or similar)
- [ ] Copy output video to persistent location: `/tmp/videos/{videoId}_overlay.mp4`
- [ ] Verify file exists and is readable
- [ ] Store filename and size for later retrieval
- [ ] Add aggressive logging
- [ ] Log: `[VIDEO_SAVE] âœ“ Saved overlay video: {videoId}_overlay.mp4 ({fileSize} bytes)`

**Implementation Notes:**
- Check track.py documentation for output video location
- May need to configure track.py output path via environment variable
- Use `fs.copyFileSync()` or `fs.renameSync()` to move file
- Handle race conditions (file still being written)
- Log file path and size for debugging

**Testing:**
```bash
# After subprocess completes, verify videos exist
ls -lh /tmp/videos/v_*_overlay.mp4
```

---

### Task 1.4: Implement Output File Detection

**Objective:** Locate the `.pkl` output file generated by track.py

**Acceptance Criteria:**
- [ ] After subprocess completes, search for `.pkl` file
- [ ] Search in `/tmp/phalp_output` directory (or track.py's default output)
- [ ] Use glob pattern to find `.pkl` files
- [ ] Return error if no `.pkl` file found
- [ ] Return error if multiple `.pkl` files found (ambiguous)
- [ ] Successfully identify single `.pkl` file
- [ ] Return full path to `.pkl` file
- [ ] Add aggressive logging
- [ ] Log: `[PICKLE_DETECT] âœ“ Found pickle file: {pklPath}`

**Implementation Notes:**
- Use Node.js `glob` package or `fs.readdirSync()`
- Check track.py documentation for output location
- May need to configure track.py output path via environment variable
- Handle race conditions (file still being written)

**Testing:**
```bash
# After subprocess completes, verify .pkl exists
ls -la /tmp/phalp_output/
```

---

## Phase 2: Pickle Parsing Service

### Task 2.1: Create Pickle Parser Service with Camera Transformation

**Objective:** Parse PHALP's `.pkl` output into JSON format with proper 3Dâ†’Three.js coordinate transformation

**Acceptance Criteria:**
- [ ] Create file: `backend/src/services/pickleParserService.ts`
- [ ] Export function: `parsePickleToFrames(pklPath: string)`
- [ ] Load `.pkl` file using Python subprocess (pickle module)
- [ ] Extract frame count from pickle data
- [ ] For each frame, extract:
  - [ ] Frame number and timestamp
  - [ ] Person detections (track_id, confidence)
  - [ ] SMPL parameters (betas, body_pose, global_orient)
  - [ ] 3D keypoints (45 joints)
  - [ ] 2D keypoints (45 joints)
  - [ ] Camera parameters (tx, ty, tz, focal_length)
  - [ ] Bounding box (x0, y0, w, h)
- [ ] Compute mesh vertices from SMPL parameters (6890 vertices)
- [ ] Compute mesh faces (13776 faces)
- [ ] **Apply camera transformation from pose-service** (CRITICAL):
  - [ ] Use `cam_crop_to_full()` logic to convert camera from crop space to full image space
  - [ ] Apply 180Â° rotation around X-axis (OpenGL convention): flip Y and Z
  - [ ] Flip X component of camera translation (from `mesh_renderer.py`)
  - [ ] Ensure vertices are in Three.js coordinate system
- [ ] Convert all numpy arrays to JavaScript arrays (JSON serializable)
- [ ] Return array of FrameData objects with transformed coordinates
- [ ] Handle parsing errors gracefully
- [ ] Add aggressive logging at each transformation step

**Implementation Notes:**
- Use Python subprocess to load pickle (Node.js has no native pickle support)
- Create helper Python script: `backend/src/services/pickle_parser.py`
- **COPY camera transformation code from `backend/pose-service/hybrid_pose_detector.py`:**
  - `cam_crop_to_full()` function for camera space conversion
  - 180Â° X-axis rotation logic from `mesh_renderer.py`
  - X-component flip: `camera_translation_adjusted[0] *= -1.0`
- SMPL model computation can use existing Python libraries
- Ensure no numpy types in output (convert to plain JS arrays)
- Handle ghost detections (tracking_confidence < 1.0)
- Log transformation parameters for debugging

**Logging:**
```
[PARSING] âœ“ Loaded pickle file: {frameCount} frames
[PARSING] âœ“ Frame 0: extracted vertices, faces, camera
[TRANSFORM] âœ“ Applied cam_crop_to_full() conversion
[TRANSFORM] âœ“ Applied 180Â° X-axis rotation
[TRANSFORM] âœ“ Flipped X component of camera translation
[TRANSFORM] âœ“ Frame 0 ready for Three.js rendering
```

**Testing:**
```typescript
// Test JSON serialization
const frames = await parsePickleToFrames(pklPath);
console.log(frames.length);  // Should be > 0
console.log(frames[0].frameNumber);  // 0
console.log(frames[0].persons[0].meshVertices.length);  // 6890
console.log(frames[0].persons[0].camera);  // Should have transformed tx, ty, tz
```

---

### Task 2.2: Create Python Pickle Parser Helper

**Objective:** Create Python script to parse pickle and output JSON

**Acceptance Criteria:**
- [ ] Create file: `backend/src/services/pickle_parser.py`
- [ ] Accept pickle file path as command-line argument
- [ ] Load pickle file using `pickle.load()`
- [ ] Extract frame data structure
- [ ] For each frame:
  - [ ] Extract frame number, timestamp
  - [ ] Extract person data (track_id, confidence, etc.)
  - [ ] Extract SMPL parameters
  - [ ] Extract keypoints (3D and 2D)
  - [ ] Extract camera parameters
  - [ ] Extract bounding box
  - [ ] Compute mesh vertices from SMPL params
  - [ ] Compute mesh faces
- [ ] Convert all numpy arrays to Python lists
- [ ] Output JSON to stdout
- [ ] Handle errors and output to stderr
- [ ] Exit with code 0 on success, 1 on error

**Implementation Notes:**
- Use `json.dumps()` for output
- Use SMPL model to compute vertices (from 4D-Humans codebase)
- Handle both dict and list pickle structures
- Ensure all output is JSON-serializable (no numpy types)

**Testing:**
```bash
python backend/src/services/pickle_parser.py /tmp/phalp_output/results.pkl | jq '.frames | length'
```

---

## Phase 3: MongoDB Frame Storage

### Task 3.1: Create Frame Storage Service

**Objective:** Store frames in MongoDB with proper schema and indexes

**Acceptance Criteria:**
- [ ] Create file: `backend/src/services/frameStorageService.ts`
- [ ] Export function: `storeFrames(videoId: string, frames: FrameData[])`
- [ ] Create MongoDB collection: `frames`
- [ ] For each frame, create document with:
  - [ ] videoId, frameNumber, timestamp
  - [ ] persons array (with all frame data)
  - [ ] createdAt, updatedAt timestamps
- [ ] Use batch insert for performance (`insertMany()`)
- [ ] Create indexes:
  - [ ] `{videoId: 1, frameNumber: 1}` (primary query)
  - [ ] `{videoId: 1}` (list all frames)
  - [ ] `{createdAt: 1}` with TTL 30 days
- [ ] Handle duplicate key errors gracefully
- [ ] Return success/error response

**Implementation Notes:**
- Use MongoDB Node.js driver
- Batch insert all frames at once (not one-by-one)
- Create indexes on first run (idempotent)
- Add error handling for connection failures

**Testing:**
```javascript
// Query single frame
db.frames.findOne({videoId: "v_123", frameNumber: 0})

// Query all frames for video
db.frames.find({videoId: "v_123"}).sort({frameNumber: 1})

// Verify document size < 16 MB
```

---

### Task 3.2: Create Frame Query Service

**Objective:** Query frames from MongoDB for frontend

**Acceptance Criteria:**
- [ ] Create file: `backend/src/services/frameQueryService.ts`
- [ ] Export function: `getFrame(videoId: string, frameNumber: number)`
- [ ] Export function: `getAllFrames(videoId: string)`
- [ ] Export function: `getFrameRange(videoId: string, start: number, end: number)`
- [ ] Return frame data with all mesh information
- [ ] Handle not found gracefully (return null)
- [ ] Handle database errors gracefully

**Implementation Notes:**
- Use MongoDB queries with proper indexes
- Return only necessary fields (optimize for frontend)
- Consider pagination for large videos

**Testing:**
```typescript
const frame = await getFrame('v_123', 0);
console.log(frame.persons[0].meshVertices.length);  // 6890

const allFrames = await getAllFrames('v_123');
console.log(allFrames.length);  // Total frame count
```

---

## Phase 4: Backend Integration

### Task 4.1: Modify finalize-upload Endpoint

**Objective:** Integrate video processing into upload flow

**Acceptance Criteria:**
- [ ] Modify file: `backend/src/server.ts` - `/api/finalize-upload` endpoint
- [ ] After assembling chunks into video file:
  - [ ] Generate videoId
  - [ ] Save video to disk: `/tmp/videos/{videoId}.mp4`
  - [ ] Spawn track.py subprocess via `processVideoWithTrackPy()`
  - [ ] If subprocess fails, return error response
  - [ ] Parse .pkl output via `parsePickleToFrames()`
  - [ ] If parsing fails, return error response
  - [ ] Store frames in MongoDB via `storeFrames()`
  - [ ] If storage fails, return error response
  - [ ] Return success response with videoId and frameCount
- [ ] Add error handling for each step
- [ ] Log timing for performance analysis
- [ ] Log all errors for debugging

**Implementation Notes:**
- Use async/await for clean flow
- Add try/catch blocks for error handling
- Log detailed error messages
- Return appropriate HTTP status codes

**Testing:**
```bash
# Upload video via API
curl -X POST http://localhost:3001/api/finalize-upload \
  -H "Content-Type: application/json" \
  -d '{"role": "user", "sessionId": "123", "filename": "test.mp4", "filesize": 1000000}'

# Should return: { success: true, videoId: "v_...", frameCount: 140 }
```

---

### Task 4.2: Create Frame Retrieval Endpoints

**Objective:** Create API endpoints for frontend to retrieve frame data

**Acceptance Criteria:**
- [ ] Create endpoint: `GET /api/frames/:videoId`
  - [ ] Returns all frames for video
  - [ ] Returns array of frame objects
  - [ ] Includes mesh data (vertices, faces, camera)
- [ ] Create endpoint: `GET /api/frames/:videoId/:frameNumber`
  - [ ] Returns single frame
  - [ ] Includes all mesh data
  - [ ] Returns 404 if not found
- [ ] Create endpoint: `GET /api/frames/:videoId/range?start=0&end=10`
  - [ ] Returns frame range
  - [ ] Useful for pagination
- [ ] Add error handling for all endpoints
- [ ] Add logging for debugging

**Implementation Notes:**
- Use frameQueryService for database access
- Return JSON responses
- Add proper HTTP status codes

**Testing:**
```bash
# Get all frames for video
curl http://localhost:3001/api/frames/v_123

# Get single frame
curl http://localhost:3001/api/frames/v_123/0

# Get frame range
curl http://localhost:3001/api/frames/v_123/range?start=0&end=10
```

---

## Phase 5: Error Handling

### Task 5.1: Implement Error Handling with Aggressive Logging

**Objective:** Handle all error cases gracefully with detailed logging

**Acceptance Criteria:**
- [ ] If video file is invalid, return HTTP 400 with error details
- [ ] If subprocess times out, return HTTP 500 with "Processing timeout"
- [ ] If subprocess crashes, return HTTP 500 with stderr output
- [ ] If .pkl file not found, return HTTP 500 with error
- [ ] If .pkl parsing fails, return HTTP 500 with error
- [ ] If MongoDB storage fails, return HTTP 500 with error
- [ ] All errors logged with full details and context
- [ ] Backend does NOT crash on any error
- [ ] Log format: `[PHASE] [STATUS] Message with context`

**Implementation Notes:**
- Use try/catch blocks with detailed logging
- Log full error stack traces
- Return appropriate HTTP status codes
- Include error details in response
- Log videoId, phase, and timing in every error

**Example Logging:**
```
[SUBPROCESS] âœ“ Spawned track.py for videoId=v_123
[SUBPROCESS] âœ— Timeout after 180s for videoId=v_123
[PARSING] âœ“ Parsed 140 frames from pickle
[PARSING] âœ— Failed to parse pickle: corrupted file
[STORAGE] âœ“ Stored 140 frames in MongoDB
[STORAGE] âœ— MongoDB connection failed
```

---

## Phase 5: Frontend Integration - Video & Mesh Polling

### Task 5.2: Create Video Metadata Endpoint

**Objective:** Provide video metadata for frontend display

**Acceptance Criteria:**
- [ ] Create endpoint: `GET /api/videos`
  - [ ] Returns all processed videos
  - [ ] Returns: videoId, filename, fps, duration, resolution, frameCount, createdAt
  - [ ] Sorted by createdAt (newest first)
- [ ] Create endpoint: `GET /api/videos/:videoId`
  - [ ] Returns single video metadata
  - [ ] Includes all frame count and timing info
- [ ] Add aggressive logging for each query
- [ ] Log: `[VIDEO_QUERY] âœ“ Retrieved {count} videos` or `[VIDEO_QUERY] âœ— Error: {error}`

**Implementation Notes:**
- Query MongoDB for video metadata
- Don't return full frame data (too large)
- Return only what frontend needs for display
- Cache metadata if needed

**Testing:**
```bash
# Get all videos
curl http://localhost:3001/api/videos

# Get single video
curl http://localhost:3001/api/videos/v_123
```

---

### Task 5.3: Create Frame Polling Endpoint

**Objective:** Allow frontend to poll for frame data

**Acceptance Criteria:**
- [ ] Create endpoint: `GET /api/frames/:videoId`
  - [ ] Returns all frames for video
  - [ ] Returns array of frame objects with mesh data
  - [ ] Includes: frameNumber, timestamp, vertices, faces, camera, personId
- [ ] Create endpoint: `GET /api/frames/:videoId/:frameNumber`
  - [ ] Returns single frame
  - [ ] Includes all mesh data
  - [ ] Returns 404 if not found
- [ ] Add aggressive logging
- [ ] Log: `[FRAME_POLL] âœ“ Retrieved {frameCount} frames for videoId={videoId}`
- [ ] Log: `[FRAME_POLL] âœ“ Retrieved frame {frameNumber} for videoId={videoId}`

**Implementation Notes:**
- Use frameQueryService for database access
- Return JSON responses
- Add proper HTTP status codes
- Log each poll request

**Testing:**
```bash
# Get all frames for video
curl http://localhost:3001/api/frames/v_123

# Get single frame
curl http://localhost:3001/api/frames/v_123/0
```

---

### Task 5.4: Create Video File Streaming Endpoints

**Objective:** Stream original and overlay videos to frontend

**Acceptance Criteria:**
- [ ] Create endpoint: `GET /api/videos/:videoId/original/stream`
  - [ ] Streams original uploaded video
  - [ ] Supports range requests for seeking
  - [ ] Returns proper video MIME type
- [ ] Create endpoint: `GET /api/videos/:videoId/overlay/stream`
  - [ ] Streams mesh overlay video (track.py output)
  - [ ] Supports range requests for seeking
  - [ ] Returns proper video MIME type
- [ ] Add aggressive logging
- [ ] Log: `[VIDEO_STREAM] âœ“ Streaming original video for videoId={videoId}`
- [ ] Log: `[VIDEO_STREAM] âœ“ Streaming overlay video for videoId={videoId}`

**Implementation Notes:**
- Use fs.createReadStream() for efficient streaming
- Support HTTP range requests for seeking
- Set proper Content-Type headers
- Log stream start/end

**Testing:**
```bash
# Stream original video
curl http://localhost:3001/api/videos/v_123/original/stream > original.mp4

# Stream overlay video
curl http://localhost:3001/api/videos/v_123/overlay/stream > overlay.mp4
```

---

## Phase 6: Frontend - Side-by-Side Layout

### Task 6.1: Create Frontend Video Display Component

**Objective:** Display original and overlay videos side-by-side

**Acceptance Criteria:**
- [ ] Create component: `VideoPanel.tsx`
- [ ] Display two video players side-by-side
- [ ] Left video: toggle between original and overlay
- [ ] Right video: always show overlay (or mesh)
- [ ] Shared controls: play/pause, seek, frame counter
- [ ] Sync both videos to same frame
- [ ] Add aggressive logging to console
- [ ] Log: `[VIDEO_PANEL] âœ“ Loaded video {videoId}`
- [ ] Log: `[VIDEO_PANEL] âœ“ Toggled to {videoType} video`
- [ ] Log: `[VIDEO_PANEL] âœ“ Seeked to frame {frameNumber}`

**Implementation Notes:**
- Use HTML5 video elements
- Implement frame-by-frame navigation
- Keep both videos in sync
- Log all user interactions

**Testing:**
```bash
# Open frontend and verify:
# - Two video players visible
# - Toggle button works
# - Play/pause syncs both videos
# - Seek works on both videos
```

---

### Task 6.2: Create Frontend Three.js Mesh Component

**Objective:** Render 3D mesh in Three.js alongside videos

**Acceptance Criteria:**
- [ ] Create component: `MeshPanel.tsx`
- [ ] Initialize Three.js scene with camera and lighting
- [ ] Load frame data from backend
- [ ] For each frame:
  - [ ] Create BufferGeometry from vertices and faces
  - [ ] Create mesh with material
  - [ ] Apply camera parameters (tx, ty, tz, focal_length)
  - [ ] Render mesh
- [ ] Sync mesh frame with video frame
- [ ] Add aggressive logging to console
- [ ] Log: `[MESH_PANEL] âœ“ Initialized Three.js scene`
- [ ] Log: `[MESH_PANEL] âœ“ Loaded {frameCount} frames`
- [ ] Log: `[MESH_PANEL] âœ“ Rendered frame {frameNumber}`

**Implementation Notes:**
- Use Three.js for 3D rendering
- Create BufferGeometry from mesh data
- Apply camera transformations
- Update mesh on frame change
- Log rendering performance

**Testing:**
```bash
# Open frontend and verify:
# - Three.js canvas visible
# - Mesh renders correctly
# - Mesh updates with video frame
# - Camera angle is correct
```

---

### Task 6.3: Create Frontend Frame Synchronization

**Objective:** Keep videos and mesh in perfect sync

**Acceptance Criteria:**
- [ ] Implement shared frame state
- [ ] When video frame changes, update mesh frame
- [ ] When mesh frame changes, update video frame
- [ ] Support play/pause on both
- [ ] Support seek on both
- [ ] Support frame-by-frame navigation
- [ ] Add aggressive logging
- [ ] Log: `[SYNC] âœ“ Frame {frameNumber} synced across all components`
- [ ] Log: `[SYNC] âœ— Sync lost at frame {frameNumber}`

**Implementation Notes:**
- Use React state for frame synchronization
- Implement event listeners for video/mesh changes
- Handle edge cases (seek, pause, play)
- Log sync state changes

**Testing:**
```bash
# Open frontend and verify:
# - Play both videos and mesh together
# - Seek in one video, both update
# - Frame counter matches all components
# - No sync drift over time
```

---

### Task 6.4: Create Frontend Mesh Display Controls

**Objective:** Add controls for mesh visualization

**Acceptance Criteria:**
- [ ] Add rotation controls (mouse drag)
- [ ] Add zoom controls (mouse wheel)
- [ ] Add reset view button
- [ ] Add wireframe toggle
- [ ] Add lighting controls (optional)
- [ ] Add aggressive logging
- [ ] Log: `[MESH_CONTROLS] âœ“ Rotated mesh`
- [ ] Log: `[MESH_CONTROLS] âœ“ Zoomed to {zoomLevel}`
- [ ] Log: `[MESH_CONTROLS] âœ“ Toggled wireframe`

**Implementation Notes:**
- Use Three.js OrbitControls
- Implement mouse event handlers
- Add UI buttons for controls
- Log all user interactions

**Testing:**
```bash
# Open frontend and verify:
# - Can rotate mesh with mouse
# - Can zoom with mouse wheel
# - Reset view button works
# - Wireframe toggle works
```

---

### Task 6.5: Create Frontend Video List & Selection

**Objective:** Allow frontend to browse and select videos

**Acceptance Criteria:**
- [ ] Create component: `VideoList.tsx`
- [ ] Poll `/api/videos` endpoint
- [ ] Display list of all processed videos
- [ ] Show: filename, duration, frameCount, createdAt
- [ ] Click to select video
- [ ] Load selected video in side-by-side view
- [ ] Add aggressive logging
- [ ] Log: `[VIDEO_LIST] âœ“ Polled {count} videos`
- [ ] Log: `[VIDEO_LIST] âœ“ Selected video {videoId}`

**Implementation Notes:**
- Use React hooks for polling
- Implement auto-refresh (every 5 seconds)
- Show loading state while polling
- Log poll frequency and results

**Testing:**
```bash
# Open frontend and verify:
# - Video list displays
# - Videos update automatically
# - Can select video
# - Selected video loads in viewer
```

---

### Task 6.6: Create Frontend Mesh Data Loading

**Objective:** Load mesh data from backend and prepare for rendering

**Acceptance Criteria:**
- [ ] When video is selected, poll `/api/frames/:videoId`
- [ ] Load all frame data
- [ ] Parse mesh data (vertices, faces, camera)
- [ ] Prepare for Three.js rendering
- [ ] Handle large frame counts (100+ frames)
- [ ] Add aggressive logging
- [ ] Log: `[MESH_LOADER] âœ“ Loading {frameCount} frames for videoId={videoId}`
- [ ] Log: `[MESH_LOADER] âœ“ Loaded frame {frameNumber}: {vertexCount} vertices, {faceCount} faces`
- [ ] Log: `[MESH_LOADER] âœ“ All frames loaded, ready for rendering`

**Implementation Notes:**
- Use async/await for loading
- Show loading progress
- Handle network errors
- Cache loaded frames
- Log loading progress

**Testing:**
```bash
# Open frontend and verify:
# - Mesh data loads when video selected
# - Progress indicator shows
# - All frames loaded successfully
# - No memory issues with large videos
```

---

## Phase 7: Configuration & Logging

### Task 7.1: Add Backend Aggressive Logging

**Objective:** Add detailed logging at every step

**Acceptance Criteria:**
- [ ] Log format: `[PHASE] [STATUS] Message`
- [ ] Log at subprocess spawn: `[SUBPROCESS] âœ“ Spawned track.py`
- [ ] Log at pickle parsing: `[PARSING] âœ“ Parsed {frameCount} frames`
- [ ] Log at MongoDB storage: `[STORAGE] âœ“ Stored {frameCount} frames`
- [ ] Log at frame queries: `[QUERY] âœ“ Retrieved {frameCount} frames`
- [ ] Log at video streaming: `[STREAM] âœ“ Streaming video`
- [ ] All logs include videoId, timing, and context
- [ ] Logs visible in console and backend logs

**Implementation Notes:**
- Use existing logger service
- Add console.log() for immediate visibility
- Include timestamps
- Include videoId in all logs
- Include timing information

**Example:**
```
[FINALIZE] ========================================
[FINALIZE] ðŸš€ FINALIZE-UPLOAD ENDPOINT CALLED
[FINALIZE] Generated videoId: v_1704067200000_1
[SUBPROCESS] âœ“ Spawned track.py for videoId=v_1704067200000_1
[SUBPROCESS] âœ“ Subprocess completed in 95s
[PARSING] âœ“ Parsing pickle file
[PARSING] âœ“ Parsed 140 frames
[STORAGE] âœ“ Storing 140 frames in MongoDB
[STORAGE] âœ“ All frames stored successfully
[FINALIZE] ========================================
[FINALIZE] âœ“âœ“âœ“ FINALIZE-UPLOAD COMPLETE âœ“âœ“âœ“
```

---

### Task 7.2: Add Frontend Aggressive Logging

**Objective:** Add detailed logging to frontend components

**Acceptance Criteria:**
- [ ] Log format: `[COMPONENT] [STATUS] Message`
- [ ] Log at component mount: `[VIDEO_LIST] âœ“ Mounted`
- [ ] Log at polling: `[VIDEO_LIST] âœ“ Polled {count} videos`
- [ ] Log at video selection: `[VIDEO_PANEL] âœ“ Selected videoId={videoId}`
- [ ] Log at mesh loading: `[MESH_LOADER] âœ“ Loading frames`
- [ ] Log at frame sync: `[SYNC] âœ“ Frame {frameNumber} synced`
- [ ] All logs visible in browser console
- [ ] Include timing information

**Implementation Notes:**
- Use console.log() for all logging
- Include component name in all logs
- Include videoId and frameNumber
- Include timing information
- Use emoji for visual clarity (âœ“, âœ—, ðŸš€, etc.)

**Example:**
```
[VIDEO_LIST] âœ“ Mounted
[VIDEO_LIST] ðŸ”„ Polling /api/videos
[VIDEO_LIST] âœ“ Polled 3 videos
[VIDEO_PANEL] âœ“ Selected videoId=v_1704067200000_1
[MESH_LOADER] ðŸš€ Loading frames for videoId=v_1704067200000_1
[MESH_LOADER] âœ“ Loaded frame 0: 6890 vertices, 13776 faces
[MESH_LOADER] âœ“ Loaded frame 1: 6890 vertices, 13776 faces
[MESH_LOADER] âœ“ All 140 frames loaded
[MESH_PANEL] âœ“ Initialized Three.js scene
[MESH_PANEL] âœ“ Rendered frame 0
[SYNC] âœ“ Frame 0 synced across all components
```

---

## Task Dependencies

```
Phase 1: Video Processing Service
â”œâ”€ Task 1.1: Create video processing service
â””â”€ Task 1.2: Implement pickle file detection

Phase 2: Pickle Parsing Service
â”œâ”€ Task 2.1: Create pickle parser service
â””â”€ Task 2.2: Create Python pickle parser helper

Phase 3: MongoDB Frame Storage
â”œâ”€ Task 3.1: Create frame storage service
â””â”€ Task 3.2: Create frame query service

Phase 4: Backend Integration
â”œâ”€ Task 4.1: Modify finalize-upload endpoint
â””â”€ Task 4.2: Create frame retrieval endpoints

Phase 5: Error Handling & Fallback
â”œâ”€ Task 5.1: Implement error handling
â””â”€ Task 5.2: Implement fallback (optional)

Phase 6: Testing & Validation
â”œâ”€ Task 6.1: Write unit tests
â”œâ”€ Task 6.2: Write property-based tests
â””â”€ Task 6.3: End-to-end testing

Phase 7: Performance & Optimization
â”œâ”€ Task 7.1: Performance benchmarking
â””â”€ Task 7.2: Optimize pickle parsing

Phase 8: Configuration & Deployment
â”œâ”€ Task 8.1: Add configuration
â””â”€ Task 8.2: Add monitoring & logging
```

## Success Criteria Summary

- [ ] Subprocess spawning implemented and functional
- [ ] `.pkl` output parsed to JSON format
- [ ] 100% frame coverage achieved (0 frames lost)
- [ ] Temporal coherence verified (smooth motion in rendered mesh)
- [ ] Output format compatible with MongoDB schema
- [ ] Backend integration complete (finalize-upload uses direct processing)
- [ ] Error handling robust (no backend crashes)
- [ ] Multi-person tracking produces consistent track IDs
- [ ] Processing time under 2 minutes for 140-frame video with GPU
- [ ] All frames stored in MongoDB and queryable by frontend
- [ ] Frame retrieval endpoints working
- [ ] Unit tests passing
- [ ] Property-based tests passing
- [ ] End-to-end testing successful
- [ ] Performance benchmarks documented
- [ ] Configuration complete and documented
- [ ] Monitoring and logging in place

